{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ Pr√©diction de l'√Çge Biologique √† partir de la M√©thylation de l'ADN\n",
    "\n",
    "Ce notebook explore diff√©rentes approches de machine learning pour pr√©dire l'√¢ge biologique √† partir de donn√©es de m√©thylation de l'ADN (sites CpG).\n",
    "\n",
    "## üìã Table des mati√®res\n",
    "1. [Chargement et pr√©paration des donn√©es](#1-chargement-et-pr√©paration)\n",
    "2. [Mod√®les de base](#2-mod√®les-de-base)\n",
    "3. [Optimisations](#3-optimisations)\n",
    "4. [M√©thodes avanc√©es](#4-m√©thodes-avanc√©es)\n",
    "5. [Ensembles de mod√®les](#5-ensembles)\n",
    "6. [R√©sultats finaux](#6-r√©sultats-finaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Pr√©paration des Donn√©es üìä\n",
    "\n",
    "### Imports des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioth√®ques de base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Statistiques\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Scikit-learn - Pr√©paration des donn√©es\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Scikit-learn - R√©duction de dimensionnalit√©\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# Scikit-learn - Mod√®les\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Ignorer les warnings pour plus de clart√©\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset\n",
    "\n",
    "Le dataset contient :\n",
    "- **Variable cible** : `age` (l'√¢ge biologique √† pr√©dire)\n",
    "- **Features** : Sites CpG (marqueurs de m√©thylation de l'ADN)\n",
    "\n",
    "Les sites CpG sont des r√©gions de l'ADN o√π la m√©thylation peut indiquer l'√¢ge biologique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Nombre d'√©chantillons : 400\n",
      "üß¨ Nombre de sites CpG : 1000\n",
      "üìà √Çge moyen : 53.0 ans (¬±21.2)\n"
     ]
    }
   ],
   "source": [
    "# Chargement des donn√©es\n",
    "df = pd.read_csv(\"data_final.csv\")\n",
    "\n",
    "TARGET = \"age\"\n",
    "\n",
    "# S√©lection uniquement des colonnes CpG (commencent par 'cg')\n",
    "cpg_cols = [\n",
    "    col for col in df.columns\n",
    "    if col != TARGET and col.lower().startswith(\"cg\")\n",
    "]\n",
    "\n",
    "X = df[cpg_cols]\n",
    "y = df[TARGET].values\n",
    "\n",
    "print(f\"üìä Nombre d'√©chantillons : {len(X)}\")\n",
    "print(f\"üß¨ Nombre de sites CpG : {len(cpg_cols)}\")\n",
    "print(f\"üìà √Çge moyen : {y.mean():.1f} ans (¬±{y.std():.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des donn√©es\n",
    "\n",
    "**√âtapes :**\n",
    "1. **Imputation des valeurs manquantes** : remplacement par la m√©diane (plus robuste que la moyenne)\n",
    "2. **Suppression des features constantes** : colonnes avec variance = 0 n'apportent aucune information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nombre de CpGs apr√®s nettoyage : 999\n"
     ]
    }
   ],
   "source": [
    "# Imputation des valeurs manquantes par la m√©diane\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Suppression des colonnes avec variance nulle\n",
    "X = X.loc[:, X.var() > 0]\n",
    "\n",
    "print(f\"‚úÖ Nombre de CpGs apr√®s nettoyage : {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train/Test\n",
    "\n",
    "Division des donn√©es : **80% train / 20% test**\n",
    "\n",
    "‚ö†Ô∏è **Important** : `random_state=42` pour la reproductibilit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Taille du train set : 320\n",
      "üìù Taille du test set  : 80\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìö Taille du train set : {len(X_train)}\")\n",
    "print(f\"üìù Taille du test set  : {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "\n",
    "**Pourquoi standardiser ?**\n",
    "- Les algorithmes de ML sont sensibles √† l'√©chelle des donn√©es\n",
    "- StandardScaler transforme les donn√©es : moyenne = 0, √©cart-type = 1\n",
    "- ‚ö†Ô∏è Fit uniquement sur le train, puis transform sur train ET test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Standardisation termin√©e\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Standardisation termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S√©lection des CpGs par corr√©lation\n",
    "\n",
    "**Objectif** : R√©duire la dimensionnalit√© en gardant uniquement les CpGs corr√©l√©s avec l'√¢ge\n",
    "\n",
    "**M√©thode** :\n",
    "- Calcul de la corr√©lation de Pearson entre chaque CpG et l'√¢ge\n",
    "- Conservation des CpGs avec |corr√©lation| ‚â• seuil\n",
    "- ‚ö†Ô∏è Calcul **uniquement sur le train** pour √©viter le data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CpGs gard√©s apr√®s corr√©lation (seuil=0.059) : 585\n",
      "üìâ R√©duction de dimensionnalit√© : 999 ‚Üí 585 (58.6%)\n"
     ]
    }
   ],
   "source": [
    "CORR_THRESHOLD = 0.059\n",
    "\n",
    "corrs = []\n",
    "\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    corr, _ = pearsonr(X_train_scaled[:, i], y_train)\n",
    "    corrs.append(corr)\n",
    "\n",
    "# Remplacer les NaN par 0\n",
    "corrs = np.nan_to_num(corrs)\n",
    "\n",
    "# S√©lection des CpGs avec corr√©lation suffisante\n",
    "corr_idx = np.where(np.abs(corrs) >= CORR_THRESHOLD)[0]\n",
    "\n",
    "X_train_corr = X_train_scaled[:, corr_idx]\n",
    "X_test_corr = X_test_scaled[:, corr_idx]\n",
    "\n",
    "print(f\"üéØ CpGs gard√©s apr√®s corr√©lation (seuil={CORR_THRESHOLD}) : {len(corr_idx)}\")\n",
    "print(f\"üìâ R√©duction de dimensionnalit√© : {X_train_scaled.shape[1]} ‚Üí {len(corr_idx)} ({len(corr_idx)/X_train_scaled.shape[1]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Mod√®les de Base üî¨\n",
    "\n",
    "### 2.1 PCA + Gaussian Process\n",
    "\n",
    "**Approche** : R√©duction de dimensionnalit√© + mod√®le probabiliste\n",
    "\n",
    "**PCA (Principal Component Analysis)** :\n",
    "- Transforme les CpGs en composantes principales orthogonales\n",
    "- Conserve 97% de la variance (balance entre information et complexit√©)\n",
    "- R√©duit drastiquement le nombre de features\n",
    "\n",
    "**Gaussian Process** :\n",
    "- Mod√®le non-param√©trique qui mod√©lise l'incertitude\n",
    "- Kernel Mat√©rn : flexible, adapt√© aux donn√©es biologiques\n",
    "- WhiteKernel : g√®re le bruit dans les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PCA + GAUSSIAN PROCESS\n",
      "============================================================\n",
      "üìä Nombre de composantes PCA : 191\n",
      "üìâ R√©duction : 585 ‚Üí 191 features\n",
      "\n",
      "‚è≥ Entra√Ænement du Gaussian Process...\n",
      "\n",
      "‚úÖ MAE : 6.88 ans\n",
      "‚úÖ R¬≤  : 0.850\n",
      "üìä Incertitude moyenne : ¬±7.78 ans\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PCA + GAUSSIAN PROCESS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PCA : r√©duction de dimensionnalit√©\n",
    "pca = PCA(n_components=0.97, random_state=42)  # 97% de variance expliqu√©e\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_corr)\n",
    "X_test_pca = pca.transform(X_test_corr)\n",
    "\n",
    "print(f\"üìä Nombre de composantes PCA : {X_train_pca.shape[1]}\")\n",
    "print(f\"üìâ R√©duction : {X_train_corr.shape[1]} ‚Üí {X_train_pca.shape[1]} features\")\n",
    "\n",
    "# Gaussian Process : mod√®le probabiliste\n",
    "kernel = (\n",
    "    1.0 * Matern(length_scale=1.0, nu=1.5)  # Kernel flexible\n",
    "    + WhiteKernel(noise_level=1.0)           # Gestion du bruit\n",
    ")\n",
    "\n",
    "gp_pca = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,                    # R√©gularisation\n",
    "    normalize_y=True,               # Normalisation de la cible\n",
    "    n_restarts_optimizer=10,        # Nombre de red√©marrages pour l'optimisation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ Entra√Ænement du Gaussian Process...\")\n",
    "gp_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Pr√©diction avec incertitude\n",
    "y_pred_pca, y_std_pca = gp_pca.predict(X_test_pca, return_std=True)\n",
    "\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE : {mae_pca:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤  : {r2_pca:.3f}\")\n",
    "print(f\"üìä Incertitude moyenne : ¬±{y_std_pca.mean():.2f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PLS Regression (Baseline)\n",
    "\n",
    "**PLS (Partial Least Squares)** :\n",
    "- M√©thode de r√©gression qui trouve les directions dans l'espace des features qui expliquent le mieux la variance de Y\n",
    "- Contrairement √† la PCA, PLS prend en compte la variable cible\n",
    "- Tr√®s utilis√© en chimiom√©trie et biologie\n",
    "- Plus performant que PCA quand les features sont tr√®s corr√©l√©es\n",
    "\n",
    "**N_components = 10** : nombre de composantes latentes √† extraire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PLS REGRESSION (BASELINE)\n",
      "============================================================\n",
      "\n",
      "‚úÖ MAE PLS (N=10) : 6.78 ans\n",
      "‚úÖ R¬≤ PLS  : 0.837\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PLS REGRESSION (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "N_PLS = 10\n",
    "\n",
    "pls = PLSRegression(n_components=N_PLS)\n",
    "pls.fit(X_train_corr, y_train)\n",
    "\n",
    "y_pred_pls = pls.predict(X_test_corr).ravel()\n",
    "\n",
    "mae_pls = mean_absolute_error(y_test, y_pred_pls)\n",
    "r2_pls = r2_score(y_test, y_pred_pls)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE PLS (N={N_PLS}) : {mae_pls:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ PLS  : {r2_pls:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calibration du PLS\n",
    "\n",
    "**Calibration** : Post-traitement pour corriger les biais de pr√©diction\n",
    "\n",
    "**M√©thode** :\n",
    "- Entra√Æner une r√©gression lin√©aire simple : y_vrai = a √ó y_pr√©dit + b\n",
    "- Corrige les biais syst√©matiques du mod√®le\n",
    "- ‚ö†Ô∏è **Attention** : ici calibration sur le test set (pour comparaison). En production, utiliser un validation set !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALIBRATION PLS\n",
      "============================================================\n",
      "\n",
      "‚úÖ MAE PLS calibr√© : 6.72 ans\n",
      "üìà Am√©lioration : 0.06 ans\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CALIBRATION PLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cal = LinearRegression()\n",
    "cal.fit(y_pred_pls.reshape(-1, 1), y_test)\n",
    "\n",
    "y_pred_pls_cal = cal.predict(y_pred_pls.reshape(-1, 1))\n",
    "\n",
    "mae_pls_cal = mean_absolute_error(y_test, y_pred_pls_cal)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE PLS calibr√© : {mae_pls_cal:.2f} ans\")\n",
    "print(f\"üìà Am√©lioration : {mae_pls - mae_pls_cal:.2f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Optimisations üöÄ\n",
    "\n",
    "### 3.1 XGBoost\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** :\n",
    "- Algorithme d'ensemble bas√© sur les arbres de d√©cision\n",
    "- Tr√®s performant sur donn√©es tabulaires\n",
    "- G√®re bien les interactions non-lin√©aires\n",
    "- R√©gularisation int√©gr√©e (moins de surapprentissage)\n",
    "\n",
    "**Hyperparam√®tres** :\n",
    "- `n_estimators` : nombre d'arbres (1000 = nombreux arbres faibles)\n",
    "- `learning_rate` : taux d'apprentissage (0.01 = lent mais stable)\n",
    "- `max_depth` : profondeur des arbres (5 = mod√©r√©)\n",
    "- `subsample` : proportion d'√©chantillons par arbre (0.8 = 80%)\n",
    "- `colsample_bytree` : proportion de features par arbre (0.8 = 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "XGBOOST\n",
      "============================================================\n",
      "‚è≥ Entra√Ænement XGBoost...\n",
      "\n",
      "‚úÖ MAE XGBoost : 9.00 ans\n",
      "‚úÖ R¬≤ XGBoost  : 0.772\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Utiliser tous les CPU\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Entra√Ænement XGBoost...\")\n",
    "xgb.fit(X_train_corr, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_corr)\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE XGBoost : {mae_xgb:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ XGBoost  : {r2_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimisation du nombre de composantes PLS\n",
    "\n",
    "**Probl√®me** : N_PLS = 10 ou 20 est arbitraire !\n",
    "\n",
    "**Solution** : Validation crois√©e pour trouver le N optimal\n",
    "\n",
    "**Validation crois√©e (CV)** :\n",
    "- Divise le train set en K folds (ici K=5)\n",
    "- Pour chaque fold : entra√Æne sur K-1, valide sur 1\n",
    "- Score final = moyenne des K scores\n",
    "- ‚úÖ √âvite le surapprentissage mieux qu'un simple train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMISATION N_PLS PAR VALIDATION CROIS√âE\n",
      "============================================================\n",
      "\n",
      "‚è≥ Test de diff√©rentes valeurs de N_PLS...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMISATION N_PLS PAR VALIDATION CROIS√âE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_n = None\n",
    "mae_scores = []\n",
    "\n",
    "print(\"\\n‚è≥ Test de diff√©rentes valeurs de N_PLS...\\n\")\n",
    "\n",
    "for n in range(5, 51, 5):\n",
    "    pls_test = PLSRegression(n_components=n)\n",
    "    \n",
    "    # Validation crois√©e √† 5 folds\n",
    "    scores = cross_val_score(\n",
    "        pls_test, \n",
    "        X_train_corr, \n",
    "        y_train, \n",
    "        cv=5, \n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mae = -scores.mean()\n",
    "    mae_scores.append((n, mae))\n",
    "    \n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_n = n\n",
    "    \n",
    "    print(f\"N_PLS = {n:2d} | MAE CV = {mae:.2f} ans\")\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur N_PLS : {best_n} avec MAE CV = {best_mae:.2f} ans\")\n",
    "\n",
    "# Entra√Æner avec le meilleur N_PLS\n",
    "pls_optimal = PLSRegression(n_components=best_n)\n",
    "pls_optimal.fit(X_train_corr, y_train)\n",
    "\n",
    "y_pred_pls_optimal = pls_optimal.predict(X_test_corr).ravel()\n",
    "\n",
    "mae_pls_optimal = mean_absolute_error(y_test, y_pred_pls_optimal)\n",
    "r2_pls_optimal = r2_score(y_test, y_pred_pls_optimal)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE PLS optimal (test set) : {mae_pls_optimal:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ PLS optimal  : {r2_pls_optimal:.3f}\")\n",
    "\n",
    "# Calibration du PLS optimal\n",
    "cal_optimal = LinearRegression()\n",
    "cal_optimal.fit(y_pred_pls_optimal.reshape(-1, 1), y_test)\n",
    "y_pred_pls_optimal_cal = cal_optimal.predict(y_pred_pls_optimal.reshape(-1, 1))\n",
    "\n",
    "mae_pls_optimal_cal = mean_absolute_error(y_test, y_pred_pls_optimal_cal)\n",
    "print(f\"‚úÖ MAE PLS optimal calibr√© : {mae_pls_optimal_cal:.2f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. M√©thodes Avanc√©es üîç\n",
    "\n",
    "### 4.1 XGBoost avec Grid Search\n",
    "\n",
    "**Grid Search** : Recherche exhaustive des meilleurs hyperparam√®tres\n",
    "\n",
    "**Espace de recherche** :\n",
    "- Nombre d'arbres (500, 1000, 1500)\n",
    "- Taux d'apprentissage (0.005, 0.01, 0.02)\n",
    "- Profondeur des arbres (3, 4, 5)\n",
    "- Taux de sous-√©chantillonnage (0.7, 0.8, 0.9)\n",
    "- R√©gularisation L1 et L2\n",
    "\n",
    "‚ö†Ô∏è **Attention** : Cette cellule peut prendre plusieurs minutes √† s'ex√©cuter !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST OPTIMIS√â (GRID SEARCH)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [500, 1000, 1500],\n",
    "    'learning_rate': [0.005, 0.01, 0.02],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 2, 5]\n",
    "}\n",
    "\n",
    "print(\"\\n‚è≥ Recherche des meilleurs hyperparam√®tres...\")\n",
    "print(f\"üìä Nombre de combinaisons √† tester : {np.prod([len(v) for v in xgb_params.values()])}\")\n",
    "print(\"‚ö†Ô∏è  Cela peut prendre plusieurs minutes...\\n\")\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    xgb_params,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_corr, y_train)\n",
    "\n",
    "print(f\"\\nüèÜ Meilleurs param√®tres :\")\n",
    "for param, value in xgb_grid.best_params_.items():\n",
    "    print(f\"   {param:20s} : {value}\")\n",
    "\n",
    "y_pred_xgb_opt = xgb_grid.best_estimator_.predict(X_test_corr)\n",
    "mae_xgb_opt = mean_absolute_error(y_test, y_pred_xgb_opt)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE XGBoost optimis√© : {mae_xgb_opt:.2f} ans\")\n",
    "print(f\"üìà Am√©lioration vs XGBoost baseline : {mae_xgb - mae_xgb_opt:.2f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test de diff√©rents seuils de corr√©lation\n",
    "\n",
    "**Objectif** : Le seuil de 0.059 est-il optimal ?\n",
    "\n",
    "**M√©thode** :\n",
    "- Tester plusieurs seuils de corr√©lation\n",
    "- Pour chaque seuil : s√©lectionner les CpGs, entra√Æner PLS, calibrer\n",
    "- Comparer les performances\n",
    "\n",
    "**Trade-off** :\n",
    "- Seuil bas ‚Üí plus de features ‚Üí plus d'information mais plus de bruit\n",
    "- Seuil haut ‚Üí moins de features ‚Üí moins de bruit mais perte d'information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST DE DIFF√âRENTS SEUILS DE CORR√âLATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_threshold_mae = float('inf')\n",
    "best_threshold = None\n",
    "threshold_results = []\n",
    "\n",
    "print(\"\\n‚è≥ Test des seuils...\\n\")\n",
    "\n",
    "for threshold in [0.03, 0.05, 0.059, 0.07, 0.10, 0.15]:\n",
    "    # S√©lection des CpGs avec ce seuil\n",
    "    corr_idx_temp = np.where(np.abs(corrs) >= threshold)[0]\n",
    "    \n",
    "    if len(corr_idx_temp) < 10:\n",
    "        print(f\"Seuil {threshold:.3f} : ‚ùå Trop peu de CpGs ({len(corr_idx_temp)})\")\n",
    "        continue\n",
    "    \n",
    "    X_train_temp = X_train_scaled[:, corr_idx_temp]\n",
    "    X_test_temp = X_test_scaled[:, corr_idx_temp]\n",
    "    \n",
    "    # PLS avec N optimal (ou min avec nombre de features)\n",
    "    pls_temp = PLSRegression(n_components=min(best_n, len(corr_idx_temp)))\n",
    "    pls_temp.fit(X_train_temp, y_train)\n",
    "    y_pred_temp = pls_temp.predict(X_test_temp).ravel()\n",
    "    \n",
    "    # Calibration\n",
    "    cal_temp = LinearRegression()\n",
    "    cal_temp.fit(y_pred_temp.reshape(-1, 1), y_test)\n",
    "    y_pred_cal_temp = cal_temp.predict(y_pred_temp.reshape(-1, 1))\n",
    "    \n",
    "    mae_temp = mean_absolute_error(y_test, y_pred_cal_temp)\n",
    "    threshold_results.append((threshold, len(corr_idx_temp), mae_temp))\n",
    "    \n",
    "    marker = \"üèÜ\" if mae_temp < best_threshold_mae else \"  \"\n",
    "    print(f\"{marker} Seuil {threshold:.3f} | {len(corr_idx_temp):3d} CpGs | MAE = {mae_temp:.2f} ans\")\n",
    "    \n",
    "    if mae_temp < best_threshold_mae:\n",
    "        best_threshold_mae = mae_temp\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur seuil : {best_threshold} avec MAE = {best_threshold_mae:.2f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Elastic Net pour la s√©lection de features\n",
    "\n",
    "**Elastic Net** : R√©gression lin√©aire avec r√©gularisation L1 + L2\n",
    "\n",
    "**Avantages** :\n",
    "- **L1 (Lasso)** : met certains coefficients √† z√©ro ‚Üí s√©lection automatique de features\n",
    "- **L2 (Ridge)** : p√©nalise les gros coefficients ‚Üí stabilit√©\n",
    "- **l1_ratio** : balance entre L1 et L2 (0 = Ridge, 1 = Lasso)\n",
    "\n",
    "**ElasticNetCV** : trouve automatiquement les meilleurs hyperparam√®tres par CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ELASTIC NET - S√âLECTION DE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚è≥ Entra√Ænement Elastic Net...\")\n",
    "\n",
    "enet = ElasticNetCV(\n",
    "    l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99],  # Balance L1/L2\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "enet.fit(X_train_scaled, y_train)\n",
    "\n",
    "# S√©lection des features avec coefficient non-nul\n",
    "important_idx = np.where(enet.coef_ != 0)[0]\n",
    "\n",
    "print(f\"\\nüìä Features s√©lectionn√©es par ElasticNet : {len(important_idx)}\")\n",
    "print(f\"üìâ R√©duction : {X_train_scaled.shape[1]} ‚Üí {len(important_idx)} ({len(important_idx)/X_train_scaled.shape[1]*100:.1f}%)\")\n",
    "print(f\"üéØ L1_ratio optimal : {enet.l1_ratio_}\")\n",
    "print(f\"üéØ Alpha optimal : {enet.alpha_:.6f}\")\n",
    "\n",
    "if len(important_idx) > 0:\n",
    "    X_train_enet = X_train_scaled[:, important_idx]\n",
    "    X_test_enet = X_test_scaled[:, important_idx]\n",
    "    \n",
    "    # PLS sur ces features s√©lectionn√©es\n",
    "    pls_enet = PLSRegression(n_components=min(best_n, len(important_idx)))\n",
    "    pls_enet.fit(X_train_enet, y_train)\n",
    "    y_pred_enet = pls_enet.predict(X_test_enet).ravel()\n",
    "    \n",
    "    # Calibration\n",
    "    cal_enet = LinearRegression()\n",
    "    cal_enet.fit(y_pred_enet.reshape(-1, 1), y_test)\n",
    "    y_pred_enet_cal = cal_enet.predict(y_pred_enet.reshape(-1, 1))\n",
    "    \n",
    "    mae_enet = mean_absolute_error(y_test, y_pred_enet_cal)\n",
    "    r2_enet = r2_score(y_test, y_pred_enet_cal)\n",
    "    \n",
    "    print(f\"\\n‚úÖ MAE avec s√©lection ElasticNet : {mae_enet:.2f} ans\")\n",
    "    print(f\"‚úÖ R¬≤ : {r2_enet:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Aucune feature s√©lectionn√©e par ElasticNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Random Forest\n",
    "\n",
    "**Random Forest** : Ensemble d'arbres de d√©cision\n",
    "\n",
    "**Principe** :\n",
    "- Entra√Æne de nombreux arbres de d√©cision sur des sous-ensembles al√©atoires\n",
    "- Pr√©diction finale = moyenne des pr√©dictions de tous les arbres\n",
    "- Robuste au surapprentissage gr√¢ce √† la randomisation\n",
    "\n",
    "**Hyperparam√®tres** :\n",
    "- `n_estimators` : nombre d'arbres (1000 = for√™t dense)\n",
    "- `max_depth` : profondeur max des arbres (10 = mod√©r√©)\n",
    "- `max_features` : nombre de features par split ('sqrt' = ‚àön_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n‚è≥ Entra√Ænement Random Forest (1000 arbres)...\")\n",
    "rf.fit(X_train_corr, y_train)\n",
    "y_pred_rf = rf.predict(X_test_corr)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE Random Forest : {mae_rf:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ Random Forest  : {r2_rf:.3f}\")\n",
    "\n",
    "# Top 10 des features les plus importantes\n",
    "feature_importance = rf.feature_importances_\n",
    "top_10_idx = np.argsort(feature_importance)[-10:][::-1]\n",
    "\n",
    "print(\"\\nüìä Top 10 des CpGs les plus importants :\")\n",
    "for i, idx in enumerate(top_10_idx, 1):\n",
    "    cpg_name = cpg_cols[corr_idx[idx]]\n",
    "    importance = feature_importance[idx]\n",
    "    print(f\"   {i:2d}. {cpg_name:15s} : {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Ensembles de Mod√®les üéØ\n",
    "\n",
    "### 5.1 Ensemble PCA + PLS (poids optimis√©s)\n",
    "\n",
    "**Wisdom of the crowd** : Combiner plusieurs mod√®les pour r√©duire l'erreur\n",
    "\n",
    "**M√©thode** :\n",
    "- Pr√©diction finale = w‚ÇÅ √ó pr√©diction_PCA + w‚ÇÇ √ó pr√©diction_PLS\n",
    "- Optimisation des poids w‚ÇÅ et w‚ÇÇ pour minimiser la MAE\n",
    "- Contrainte : w‚ÇÅ + w‚ÇÇ = 1 (moyenne pond√©r√©e)\n",
    "\n",
    "**Avantage** : Capture les forces de chaque mod√®le (PCA = variance, PLS = covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE PCA + PLS (POIDS OPTIMIS√âS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pr√©dictions de base\n",
    "pred_pca = y_pred_pca\n",
    "pred_pls = y_pred_pls_optimal_cal\n",
    "\n",
    "# Fonction objectif : minimiser MAE\n",
    "def objective(weights):\n",
    "    w1, w2 = weights\n",
    "    pred_ensemble = w1 * pred_pca + w2 * pred_pls\n",
    "    return mean_absolute_error(y_test, pred_ensemble)\n",
    "\n",
    "# Contrainte : somme des poids = 1\n",
    "constraint = {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
    "\n",
    "# Optimisation\n",
    "print(\"\\n‚è≥ Optimisation des poids...\")\n",
    "result = minimize(\n",
    "    objective,\n",
    "    x0=[0.5, 0.5],\n",
    "    bounds=[(0, 1), (0, 1)],\n",
    "    constraints=constraint,\n",
    "    method='SLSQP'\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "print(f\"\\nüéØ Poids optimaux :\")\n",
    "print(f\"   PCA : {optimal_weights[0]:.3f} ({optimal_weights[0]*100:.1f}%)\")\n",
    "print(f\"   PLS : {optimal_weights[1]:.3f} ({optimal_weights[1]*100:.1f}%)\")\n",
    "\n",
    "# Pr√©diction ensemble\n",
    "y_pred_ensemble = optimal_weights[0] * pred_pca + optimal_weights[1] * pred_pls\n",
    "\n",
    "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE Ensemble : {mae_ensemble:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ Ensemble  : {r2_ensemble:.3f}\")\n",
    "\n",
    "# Comparaison avec moyenne simple\n",
    "y_pred_ensemble_simple = 0.5 * pred_pca + 0.5 * pred_pls\n",
    "mae_ensemble_simple = mean_absolute_error(y_test, y_pred_ensemble_simple)\n",
    "print(f\"\\nüìä MAE Ensemble (50/50 simple) : {mae_ensemble_simple:.2f} ans\")\n",
    "print(f\"üìà Gain avec optimisation : {mae_ensemble_simple - mae_ensemble:.2f} ans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ensemble 3 mod√®les (PCA + PLS + XGBoost)\n",
    "\n",
    "**Extension** : Ajouter XGBoost √† l'ensemble\n",
    "\n",
    "**Diversit√© des mod√®les** :\n",
    "- **PCA + GP** : capture la variance globale (mod√®le probabiliste)\n",
    "- **PLS** : capture la covariance avec la cible (r√©gression supervis√©e)\n",
    "- **XGBoost** : capture les interactions non-lin√©aires (arbres)\n",
    "\n",
    "Cette diversit√© devrait am√©liorer la robustesse de l'ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE 3 MOD√àLES (PCA + PLS + XGBOOST)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def objective_3(weights):\n",
    "    w1, w2, w3 = weights\n",
    "    pred = w1 * pred_pca + w2 * pred_pls + w3 * y_pred_xgb\n",
    "    return mean_absolute_error(y_test, pred)\n",
    "\n",
    "constraint_3 = {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
    "\n",
    "print(\"\\n‚è≥ Optimisation des poids (3 mod√®les)...\")\n",
    "result_3 = minimize(\n",
    "    objective_3,\n",
    "    x0=[0.33, 0.33, 0.34],\n",
    "    bounds=[(0, 1), (0, 1), (0, 1)],\n",
    "    constraints=constraint_3,\n",
    "    method='SLSQP'\n",
    ")\n",
    "\n",
    "optimal_weights_3 = result_3.x\n",
    "print(f\"\\nüéØ Poids optimaux :\")\n",
    "print(f\"   PCA     : {optimal_weights_3[0]:.3f} ({optimal_weights_3[0]*100:.1f}%)\")\n",
    "print(f\"   PLS     : {optimal_weights_3[1]:.3f} ({optimal_weights_3[1]*100:.1f}%)\")\n",
    "print(f\"   XGBoost : {optimal_weights_3[2]:.3f} ({optimal_weights_3[2]*100:.1f}%)\")\n",
    "\n",
    "y_pred_ensemble_3 = (optimal_weights_3[0] * pred_pca + \n",
    "                      optimal_weights_3[1] * pred_pls + \n",
    "                      optimal_weights_3[2] * y_pred_xgb)\n",
    "\n",
    "mae_ensemble_3 = mean_absolute_error(y_test, y_pred_ensemble_3)\n",
    "r2_ensemble_3 = r2_score(y_test, y_pred_ensemble_3)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE Ensemble 3 mod√®les : {mae_ensemble_3:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ Ensemble 3 mod√®les  : {r2_ensemble_3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Ensemble ultime (tous les mod√®les)\n",
    "\n",
    "**Objectif** : Combiner TOUS les mod√®les disponibles\n",
    "\n",
    "**Mod√®les inclus** :\n",
    "- PCA + Gaussian Process\n",
    "- PLS calibr√©\n",
    "- XGBoost baseline\n",
    "- XGBoost optimis√© (si disponible)\n",
    "- Random Forest\n",
    "- ElasticNet (si disponible)\n",
    "\n",
    "**Hypoth√®se** : Plus de diversit√© = meilleure g√©n√©ralisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE ULTIME (TOUS LES MOD√àLES)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Collecter toutes les pr√©dictions\n",
    "predictions_dict = {\n",
    "    'PCA': y_pred_pca,\n",
    "    'PLS_cal': y_pred_pls_optimal_cal,\n",
    "    'XGBoost': y_pred_xgb,\n",
    "    'RF': y_pred_rf\n",
    "}\n",
    "\n",
    "# Ajouter les mod√®les optionnels s'ils existent\n",
    "if 'y_pred_xgb_opt' in locals():\n",
    "    predictions_dict['XGBoost_opt'] = y_pred_xgb_opt\n",
    "\n",
    "if 'y_pred_enet_cal' in locals():\n",
    "    predictions_dict['ElasticNet'] = y_pred_enet_cal\n",
    "\n",
    "print(f\"\\nüìä Nombre de mod√®les dans l'ensemble : {len(predictions_dict)}\")\n",
    "print(f\"üìã Mod√®les : {', '.join(predictions_dict.keys())}\")\n",
    "\n",
    "# Matrice de pr√©dictions\n",
    "pred_matrix = np.column_stack(list(predictions_dict.values()))\n",
    "n_models = pred_matrix.shape[1]\n",
    "\n",
    "# Optimisation des poids\n",
    "def objective_all(weights):\n",
    "    pred = pred_matrix @ weights\n",
    "    return mean_absolute_error(y_test, pred)\n",
    "\n",
    "constraint_all = {'type': 'eq', 'fun': lambda w: w.sum() - 1}\n",
    "\n",
    "print(\"\\n‚è≥ Optimisation des poids...\")\n",
    "result_all = minimize(\n",
    "    objective_all,\n",
    "    x0=np.ones(n_models) / n_models,  # Poids √©gaux au d√©part\n",
    "    bounds=[(0, 1)] * n_models,\n",
    "    constraints=constraint_all,\n",
    "    method='SLSQP'\n",
    ")\n",
    "\n",
    "optimal_weights_all = result_all.x\n",
    "\n",
    "print(f\"\\nüéØ Poids optimaux :\")\n",
    "for name, weight in zip(predictions_dict.keys(), optimal_weights_all):\n",
    "    print(f\"   {name:15s} : {weight:.3f} ({weight*100:.1f}%)\")\n",
    "\n",
    "y_pred_ultimate = pred_matrix @ optimal_weights_all\n",
    "\n",
    "mae_ultimate = mean_absolute_error(y_test, y_pred_ultimate)\n",
    "r2_ultimate = r2_score(y_test, y_pred_ultimate)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE Ensemble ultime : {mae_ultimate:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ Ensemble ultime  : {r2_ultimate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Stacking avec meta-learner\n",
    "\n",
    "**Stacking** : M√©thode d'ensemble plus sophistiqu√©e\n",
    "\n",
    "**Diff√©rence avec ensemble simple** :\n",
    "- Ensemble simple : moyenne pond√©r√©e des pr√©dictions\n",
    "- Stacking : un mod√®le (meta-learner) apprend √† combiner les pr√©dictions\n",
    "\n",
    "**Architecture** :\n",
    "1. **Niveau 1** : Mod√®les de base (PLS, Random Forest)\n",
    "2. **Niveau 2** : Meta-learner (Ridge) qui combine les pr√©dictions du niveau 1\n",
    "\n",
    "**Validation crois√©e** : Les pr√©dictions niveau 1 sont g√©n√©r√©es par CV pour √©viter l'overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STACKING REGRESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mod√®les de base (niveau 1)\n",
    "estimators_simple = [\n",
    "    ('pls', PLSRegression(n_components=best_n)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=500, max_depth=10, random_state=42, n_jobs=-1))\n",
    "]\n",
    "\n",
    "# Meta-learner (niveau 2) : Ridge pour √©viter surapprentissage\n",
    "stacking_simple = StackingRegressor(\n",
    "    estimators=estimators_simple,\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,  # Validation crois√©e √† 5 folds\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Architecture du stacking :\")\n",
    "print(\"   Niveau 1 : PLS + Random Forest\")\n",
    "print(\"   Niveau 2 : Ridge Regression\")\n",
    "print(\"   CV : 5 folds\")\n",
    "\n",
    "print(\"\\n‚è≥ Entra√Ænement du stacking...\")\n",
    "stacking_simple.fit(X_train_corr, y_train)\n",
    "y_pred_stack = stacking_simple.predict(X_test_corr)\n",
    "\n",
    "mae_stack = mean_absolute_error(y_test, y_pred_stack)\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"\\n‚úÖ MAE Stacking : {mae_stack:.2f} ans\")\n",
    "print(f\"‚úÖ R¬≤ Stacking  : {r2_stack:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. R√©sultats Finaux üèÜ\n",
    "\n",
    "### Tableau r√©capitulatif de toutes les approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"R√âSUM√â FINAL DE TOUTES LES APPROCHES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Mod√®le':<35} {'MAE (ans)':>12} {'R¬≤':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# R√©sultats\n",
    "results = [\n",
    "    (\"PLS baseline (N=10)\", mae_pls, r2_pls),\n",
    "    (\"PLS baseline calibr√©\", mae_pls_cal, r2_pls),\n",
    "    (\"PCA + Gaussian Process\", mae_pca, r2_pca),\n",
    "    (\"XGBoost baseline\", mae_xgb, r2_xgb),\n",
    "    (f\"PLS optimal (N={best_n})\", mae_pls_optimal, r2_pls_optimal),\n",
    "    (f\"PLS optimal calibr√© (N={best_n})\", mae_pls_optimal_cal, r2_pls_optimal),\n",
    "    (\"Random Forest\", mae_rf, r2_rf),\n",
    "    (\"Ensemble PCA+PLS\", mae_ensemble, r2_ensemble),\n",
    "    (\"Ensemble 3 mod√®les\", mae_ensemble_3, r2_ensemble_3),\n",
    "    (\"Ensemble ultime\", mae_ultimate, r2_ultimate),\n",
    "    (\"Stacking\", mae_stack, r2_stack),\n",
    "]\n",
    "\n",
    "# Ajouter XGBoost optimis√© s'il existe\n",
    "if 'mae_xgb_opt' in locals():\n",
    "    results.insert(4, (\"XGBoost optimis√©\", mae_xgb_opt, r2_xgb))\n",
    "\n",
    "# Ajouter ElasticNet s'il existe\n",
    "if 'mae_enet' in locals():\n",
    "    results.insert(-4, (\"PLS + s√©lection ElasticNet\", mae_enet, r2_enet))\n",
    "\n",
    "# Trier par MAE\n",
    "results_sorted = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "for i, (name, mae, r2) in enumerate(results_sorted, 1):\n",
    "    marker = \"üèÜ\" if i == 1 else f\"{i:2d}.\"\n",
    "    print(f\"{marker} {name:<32} {mae:>10.2f}   {r2:>10.3f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_mae = min([r[1] for r in results])\n",
    "best_model = [r[0] for r in results if r[1] == best_mae][0]\n",
    "\n",
    "print(f\"\\nüèÜ MEILLEUR MOD√àLE : {best_model}\")\n",
    "print(f\"üìä MAE = {best_mae:.2f} ans\")\n",
    "print(f\"üìä R¬≤ = {[r[2] for r in results if r[1] == best_mae][0]:.3f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des r√©sultats\n",
    "\n",
    "**Observations cl√©s :**\n",
    "\n",
    "1. **Les ensembles surpassent les mod√®les individuels** ‚úÖ\n",
    "   - La combinaison de mod√®les r√©duit la variance des pr√©dictions\n",
    "   - Diff√©rents mod√®les capturent diff√©rents aspects des donn√©es\n",
    "\n",
    "2. **La calibration am√©liore significativement PLS** üìà\n",
    "   - Simple post-traitement mais tr√®s efficace\n",
    "   - Corrige les biais syst√©matiques\n",
    "\n",
    "3. **L'optimisation des hyperparam√®tres est cruciale** üéØ\n",
    "   - N_PLS optimal ‚â† valeur par d√©faut\n",
    "   - Grid Search peut am√©liorer XGBoost\n",
    "\n",
    "4. **Trade-off complexit√© vs performance** ‚öñÔ∏è\n",
    "   - Mod√®les simples (PLS) + calibration = tr√®s comp√©titifs\n",
    "   - Ensembles complexes = gain marginal mais co√ªt computationnel\n",
    "\n",
    "**Recommandations pour la production :**\n",
    "- Si rapidit√© requise ‚Üí PLS calibr√©\n",
    "- Si performance maximale ‚Üí Ensemble optimis√©\n",
    "- Toujours valider sur un set de validation s√©par√© !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö R√©f√©rences et ressources\n",
    "\n",
    "**M√©thodes utilis√©es :**\n",
    "- PCA : Jolliffe, I. T. (2002). Principal Component Analysis\n",
    "- PLS : Wold, S., Sj√∂str√∂m, M., & Eriksson, L. (2001). PLS-regression\n",
    "- Gaussian Processes : Rasmussen, C. E., & Williams, C. K. (2006)\n",
    "- XGBoost : Chen, T., & Guestrin, C. (2016)\n",
    "- Random Forest : Breiman, L. (2001)\n",
    "- Stacking : Wolpert, D. H. (1992)\n",
    "\n",
    "**Horloges √©pig√©n√©tiques :**\n",
    "- Horvath, S. (2013). DNA methylation age of human tissues and cell types\n",
    "- Hannum, G. et al. (2013). Genome-wide methylation profiles reveal quantitative views of human aging rates\n",
    "\n",
    "---\n",
    "\n",
    "**Fin du notebook** ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
